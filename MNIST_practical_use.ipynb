{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用keras做手寫數字(MNIST)辨識\n",
    "*首先預設第一版的參數,之後分別做了以下的比較\n",
    "### (1)增加神經元數量(model 2)\n",
    "### (2)batch size減少(model 3)\n",
    "### (3)層數增加(model 4)\n",
    "### (4)替換Activation function(model 5)\n",
    "### (5)做正規化(model 6)\n",
    "### 最後將以上測出的結果做統整,做優化測試(model_optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000,784)\n",
    "x_test = x_test.reshape(10000,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train = np_utils.to_categorical(y_train,10)\n",
    "y_test = np_utils.to_categorical(y_test,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 測試第一板的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 8,910\n",
      "Trainable params: 8,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0917 - acc: 0.1182 - val_loss: 0.0907 - val_acc: 0.1779\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0901 - acc: 0.2051 - val_loss: 0.0895 - val_acc: 0.2263\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0891 - acc: 0.2400 - val_loss: 0.0887 - val_acc: 0.2598\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0884 - acc: 0.2658 - val_loss: 0.0881 - val_acc: 0.2559\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0878 - acc: 0.2599 - val_loss: 0.0875 - val_acc: 0.2778\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0873 - acc: 0.2614 - val_loss: 0.0870 - val_acc: 0.2830\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0868 - acc: 0.2765 - val_loss: 0.0865 - val_acc: 0.2767\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0863 - acc: 0.2710 - val_loss: 0.0859 - val_acc: 0.2655\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0857 - acc: 0.2550 - val_loss: 0.0854 - val_acc: 0.2640\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0851 - acc: 0.2597 - val_loss: 0.0847 - val_acc: 0.2566\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0845 - acc: 0.2545 - val_loss: 0.0840 - val_acc: 0.2559\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0837 - acc: 0.2552 - val_loss: 0.0833 - val_acc: 0.2571\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0829 - acc: 0.2588 - val_loss: 0.0825 - val_acc: 0.2661\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0821 - acc: 0.2744 - val_loss: 0.0816 - val_acc: 0.2878\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0812 - acc: 0.2887 - val_loss: 0.0807 - val_acc: 0.2936\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0803 - acc: 0.2932 - val_loss: 0.0797 - val_acc: 0.2965\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0793 - acc: 0.2964 - val_loss: 0.0788 - val_acc: 0.2987\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0784 - acc: 0.3151 - val_loss: 0.0778 - val_acc: 0.3252\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0774 - acc: 0.3315 - val_loss: 0.0769 - val_acc: 0.3275\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 3s - loss: 0.0765 - acc: 0.3339 - val_loss: 0.0760 - val_acc: 0.3323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1bfb3cbdd8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model使用最基本形式\n",
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim = 784))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='mse',optimizer=SGD(lr = 0.05),metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train,y_train,batch_size=100,epochs=20,verbose = 1,validation_data = (x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (model2)將神經元*10倍,測試成效: testing accuracy可到達0.8857\n",
    "* 以下model接與此做比較\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                2010      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 100,710\n",
      "Trainable params: 100,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " 9728/10000 [============================>.] - ETA: 0s---------Training accuracy =  [0.026147572126984597, 0.8801]\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(100,input_dim = 784))\n",
    "model2.add(Activation('sigmoid'))\n",
    "model2.add(Dense(200))\n",
    "model2.add(Activation('sigmoid'))\n",
    "model2.add(Dense(10))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.compile(loss='mse',optimizer=SGD(lr = 0.05),metrics=['accuracy'])\n",
    "model2.summary()\n",
    "model2.fit(x_train,y_train,batch_size=100,epochs=20,verbose = 0)\n",
    "\n",
    "score = model2.evaluate(x_test,y_test)\n",
    "print(\"---------Training accuracy = \",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (model3)將batch調整成原本的十分之一,測試成效: testing accuracy可到達0.9403\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                2010      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 100,710\n",
      "Trainable params: 100,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " 9856/10000 [============================>.] - ETA: 0s---------Training accuracy =  [0.009306545340223238, 0.9407]\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(100,input_dim = 784))\n",
    "model3.add(Activation('sigmoid'))\n",
    "model3.add(Dense(200))\n",
    "model3.add(Activation('sigmoid'))\n",
    "model3.add(Dense(10))\n",
    "model3.add(Activation('softmax'))\n",
    "model3.compile(loss='mse',optimizer=SGD(lr = 0.05),metrics=['accuracy'])\n",
    "model3.summary()\n",
    "model3.fit(x_train,y_train,batch_size=10,epochs=20,verbose = 0)\n",
    "\n",
    "score = model3.evaluate(x_test,y_test)\n",
    "print(\"---------Training accuracy = \",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (model4)相近數量的神經元,層數變多:效果不一定變好\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 150)               15150     \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 60)                9060      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                610       \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 103,320\n",
      "Trainable params: 103,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " 9760/10000 [============================>.] - ETA: 0s---------Training accuracy =  [0.08046078827381134, 0.2405]\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Dense(100,input_dim = 784))\n",
    "model4.add(Activation('sigmoid'))\n",
    "model4.add(Dense(150))\n",
    "model4.add(Activation('sigmoid'))\n",
    "model4.add(Dense(60))\n",
    "model4.add(Activation('sigmoid'))\n",
    "model4.add(Dense(10))\n",
    "model4.add(Activation('softmax'))\n",
    "model4.compile(loss='mse',optimizer=SGD(lr = 0.05),metrics=['accuracy'])\n",
    "model4.summary()\n",
    "model4.fit(x_train,y_train,batch_size=100,epochs=20,verbose = 0)\n",
    "\n",
    "score = model4.evaluate(x_test,y_test)\n",
    "print(\"---------Training accuracy = \",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (model5)前端的Activation function改成RELU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                2010      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 100,710\n",
      "Trainable params: 100,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " 9568/10000 [===========================>..] - ETA: 0s---------Training accuracy =  [0.0668385820955038, 0.6645]\n"
     ]
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Dense(100,input_dim = 784))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dense(200))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dense(10))\n",
    "model5.add(Activation('softmax'))\n",
    "model5.compile(loss='mse',optimizer=SGD(lr = 0.05),metrics=['accuracy'])\n",
    "model5.summary()\n",
    "model5.fit(x_train,y_train,batch_size=100,epochs=20,verbose = 0)\n",
    "\n",
    "score = model5.evaluate(x_test,y_test)\n",
    "print(\"---------Training accuracy = \",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 將資料正規化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#進化版模型中對x進行normalize\n",
    "x_train_norm = x_train/x_train.max()\n",
    "x_test_norm = x_test/x_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (model6)將資料正規劃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 10)                2010      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 100,710\n",
      "Trainable params: 100,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " 9600/10000 [===========================>..] - ETA: 0s---------Training accuracy =  [0.08697155153751374, 0.3001]\n"
     ]
    }
   ],
   "source": [
    "model6 = Sequential()\n",
    "model6.add(Dense(100,input_dim = 784))\n",
    "model6.add(Activation('sigmoid'))\n",
    "model6.add(Dense(200))\n",
    "model6.add(Activation('sigmoid'))\n",
    "model6.add(Dense(10))\n",
    "model6.add(Activation('softmax'))\n",
    "model6.compile(loss='mse',optimizer=SGD(lr = 0.05),metrics=['accuracy'])\n",
    "model6.summary()\n",
    "model6.fit(x_train_norm,y_train,batch_size=100,epochs=20,verbose = 0)\n",
    "\n",
    "score = model6.evaluate(x_test_norm,y_test)\n",
    "print(\"---------Training accuracy = \",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 測試前,已知learning rate調小,最後資料在fit的時候能夠與training data有較微小的修正,不過調過小的話會造成計算時間冗長\n",
    "* 測試後發現可能讓準確率提高的因素\n",
    "### (1)增加神經元數量(o)\n",
    "### (2)batch size簡少(o)\n",
    "### (3)層數增加(不一定)\n",
    "### (4)替化Activation function(不一定)\n",
    "### (5)做正規化(不一定)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_23 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                2010      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 100,710\n",
      "Trainable params: 100,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0849 - acc: 0.2961 - val_loss: 0.0777 - val_acc: 0.4243\n",
      "Epoch 2/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0697 - acc: 0.5511 - val_loss: 0.0606 - val_acc: 0.6358\n",
      "Epoch 3/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0525 - acc: 0.7068 - val_loss: 0.0440 - val_acc: 0.7685\n",
      "Epoch 4/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0385 - acc: 0.7957 - val_loss: 0.0325 - val_acc: 0.8384\n",
      "Epoch 5/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0292 - acc: 0.8550 - val_loss: 0.0249 - val_acc: 0.8822\n",
      "Epoch 6/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0232 - acc: 0.8830 - val_loss: 0.0204 - val_acc: 0.8955\n",
      "Epoch 7/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0196 - acc: 0.8947 - val_loss: 0.0178 - val_acc: 0.9050\n",
      "Epoch 8/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0173 - acc: 0.9028 - val_loss: 0.0161 - val_acc: 0.9099\n",
      "Epoch 9/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0158 - acc: 0.9096 - val_loss: 0.0150 - val_acc: 0.9137\n",
      "Epoch 10/60\n",
      "60000/60000 [==============================] - 32s - loss: 0.0147 - acc: 0.9144 - val_loss: 0.0141 - val_acc: 0.9198\n",
      "Epoch 11/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0138 - acc: 0.9188 - val_loss: 0.0133 - val_acc: 0.9187\n",
      "Epoch 12/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0132 - acc: 0.9224 - val_loss: 0.0128 - val_acc: 0.9234\n",
      "Epoch 13/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0125 - acc: 0.9249 - val_loss: 0.0123 - val_acc: 0.9259\n",
      "Epoch 14/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0122 - acc: 0.9269 - val_loss: 0.0120 - val_acc: 0.9281\n",
      "Epoch 15/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0117 - acc: 0.9297 - val_loss: 0.0116 - val_acc: 0.9277\n",
      "Epoch 16/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0113 - acc: 0.9318 - val_loss: 0.0114 - val_acc: 0.9278\n",
      "Epoch 17/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0110 - acc: 0.9335 - val_loss: 0.0110 - val_acc: 0.9316\n",
      "Epoch 18/60\n",
      "60000/60000 [==============================] - 32s - loss: 0.0106 - acc: 0.9358 - val_loss: 0.0106 - val_acc: 0.9354\n",
      "Epoch 19/60\n",
      "60000/60000 [==============================] - 34s - loss: 0.0104 - acc: 0.9372 - val_loss: 0.0105 - val_acc: 0.9358\n",
      "Epoch 20/60\n",
      "60000/60000 [==============================] - 34s - loss: 0.0101 - acc: 0.9388 - val_loss: 0.0104 - val_acc: 0.9357\n",
      "Epoch 21/60\n",
      "60000/60000 [==============================] - 34s - loss: 0.0099 - acc: 0.9407 - val_loss: 0.0102 - val_acc: 0.9384\n",
      "Epoch 22/60\n",
      "60000/60000 [==============================] - 34s - loss: 0.0095 - acc: 0.9426 - val_loss: 0.0100 - val_acc: 0.9382\n",
      "Epoch 23/60\n",
      "60000/60000 [==============================] - 35s - loss: 0.0095 - acc: 0.9422 - val_loss: 0.0097 - val_acc: 0.9407\n",
      "Epoch 24/60\n",
      "60000/60000 [==============================] - 35s - loss: 0.0091 - acc: 0.9449 - val_loss: 0.0097 - val_acc: 0.9416\n",
      "Epoch 25/60\n",
      "60000/60000 [==============================] - 35s - loss: 0.0090 - acc: 0.9457 - val_loss: 0.0095 - val_acc: 0.9413\n",
      "Epoch 26/60\n",
      "60000/60000 [==============================] - 37s - loss: 0.0088 - acc: 0.9462 - val_loss: 0.0095 - val_acc: 0.9421\n",
      "Epoch 27/60\n",
      "60000/60000 [==============================] - 34s - loss: 0.0088 - acc: 0.9469 - val_loss: 0.0092 - val_acc: 0.9436\n",
      "Epoch 28/60\n",
      "60000/60000 [==============================] - 37s - loss: 0.0086 - acc: 0.9488 - val_loss: 0.0090 - val_acc: 0.9456\n",
      "Epoch 29/60\n",
      "60000/60000 [==============================] - 32s - loss: 0.0083 - acc: 0.9499 - val_loss: 0.0088 - val_acc: 0.9452\n",
      "Epoch 30/60\n",
      "60000/60000 [==============================] - 34s - loss: 0.0083 - acc: 0.9506 - val_loss: 0.0089 - val_acc: 0.9448\n",
      "Epoch 31/60\n",
      "60000/60000 [==============================] - 38s - loss: 0.0082 - acc: 0.9510 - val_loss: 0.0090 - val_acc: 0.9423\n",
      "Epoch 32/60\n",
      "60000/60000 [==============================] - 42s - loss: 0.0080 - acc: 0.9517 - val_loss: 0.0088 - val_acc: 0.9463\n",
      "Epoch 33/60\n",
      "60000/60000 [==============================] - 40s - loss: 0.0080 - acc: 0.9512 - val_loss: 0.0086 - val_acc: 0.9467\n",
      "Epoch 34/60\n",
      "60000/60000 [==============================] - 42s - loss: 0.0079 - acc: 0.9527 - val_loss: 0.0086 - val_acc: 0.9464\n",
      "Epoch 35/60\n",
      "60000/60000 [==============================] - 38s - loss: 0.0077 - acc: 0.9532 - val_loss: 0.0084 - val_acc: 0.9470\n",
      "Epoch 36/60\n",
      "60000/60000 [==============================] - 37s - loss: 0.0076 - acc: 0.9541 - val_loss: 0.0084 - val_acc: 0.9466\n",
      "Epoch 37/60\n",
      "60000/60000 [==============================] - 38s - loss: 0.0075 - acc: 0.9547 - val_loss: 0.0084 - val_acc: 0.9457\n",
      "Epoch 38/60\n",
      "60000/60000 [==============================] - 41s - loss: 0.0074 - acc: 0.9558 - val_loss: 0.0081 - val_acc: 0.9491\n",
      "Epoch 39/60\n",
      "60000/60000 [==============================] - 42s - loss: 0.0072 - acc: 0.9560 - val_loss: 0.0082 - val_acc: 0.9486\n",
      "Epoch 40/60\n",
      "60000/60000 [==============================] - 43s - loss: 0.0072 - acc: 0.9565 - val_loss: 0.0080 - val_acc: 0.9501\n",
      "Epoch 41/60\n",
      "60000/60000 [==============================] - 40s - loss: 0.0071 - acc: 0.9575 - val_loss: 0.0080 - val_acc: 0.9496\n",
      "Epoch 42/60\n",
      "60000/60000 [==============================] - 38s - loss: 0.0069 - acc: 0.9586 - val_loss: 0.0080 - val_acc: 0.9490\n",
      "Epoch 43/60\n",
      "60000/60000 [==============================] - 37s - loss: 0.0069 - acc: 0.9589 - val_loss: 0.0078 - val_acc: 0.9521\n",
      "Epoch 44/60\n",
      "60000/60000 [==============================] - 37s - loss: 0.0068 - acc: 0.9588 - val_loss: 0.0079 - val_acc: 0.9497\n",
      "Epoch 45/60\n",
      "60000/60000 [==============================] - 36s - loss: 0.0068 - acc: 0.9588 - val_loss: 0.0079 - val_acc: 0.9506\n",
      "Epoch 46/60\n",
      "60000/60000 [==============================] - 35s - loss: 0.0066 - acc: 0.9609 - val_loss: 0.0078 - val_acc: 0.9513\n",
      "Epoch 47/60\n",
      "60000/60000 [==============================] - 35s - loss: 0.0065 - acc: 0.9613 - val_loss: 0.0076 - val_acc: 0.9541\n",
      "Epoch 48/60\n",
      "60000/60000 [==============================] - 35s - loss: 0.0065 - acc: 0.9613 - val_loss: 0.0078 - val_acc: 0.9518\n",
      "Epoch 49/60\n",
      "60000/60000 [==============================] - 35s - loss: 0.0064 - acc: 0.9618 - val_loss: 0.0076 - val_acc: 0.9524\n",
      "Epoch 50/60\n",
      "60000/60000 [==============================] - 36s - loss: 0.0063 - acc: 0.9619 - val_loss: 0.0075 - val_acc: 0.9521\n",
      "Epoch 51/60\n",
      "60000/60000 [==============================] - 35s - loss: 0.0064 - acc: 0.9617 - val_loss: 0.0077 - val_acc: 0.9512\n",
      "Epoch 52/60\n",
      "60000/60000 [==============================] - 37s - loss: 0.0062 - acc: 0.9634 - val_loss: 0.0074 - val_acc: 0.9539\n",
      "Epoch 53/60\n",
      "60000/60000 [==============================] - 37s - loss: 0.0062 - acc: 0.9632 - val_loss: 0.0074 - val_acc: 0.9531\n",
      "Epoch 54/60\n",
      "60000/60000 [==============================] - 37s - loss: 0.0062 - acc: 0.9635 - val_loss: 0.0074 - val_acc: 0.9523\n",
      "Epoch 55/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 36s - loss: 0.0061 - acc: 0.9634 - val_loss: 0.0075 - val_acc: 0.9527\n",
      "Epoch 56/60\n",
      "60000/60000 [==============================] - 35s - loss: 0.0061 - acc: 0.9634 - val_loss: 0.0071 - val_acc: 0.9554\n",
      "Epoch 57/60\n",
      "60000/60000 [==============================] - 36s - loss: 0.0061 - acc: 0.9638 - val_loss: 0.0076 - val_acc: 0.9502\n",
      "Epoch 58/60\n",
      "60000/60000 [==============================] - 34s - loss: 0.0059 - acc: 0.9645 - val_loss: 0.0072 - val_acc: 0.9543\n",
      "Epoch 59/60\n",
      "60000/60000 [==============================] - 33s - loss: 0.0059 - acc: 0.9660 - val_loss: 0.0070 - val_acc: 0.9550\n",
      "Epoch 60/60\n",
      "60000/60000 [==============================] - 31s - loss: 0.0057 - acc: 0.9657 - val_loss: 0.0071 - val_acc: 0.9551\n",
      " 9952/10000 [============================>.] - ETA: 0s---------Training accuracy =  [0.026147572126984597, 0.8801]\n"
     ]
    }
   ],
   "source": [
    "model_optimize = Sequential()\n",
    "model_optimize.add(Dense(100,input_dim = 784))\n",
    "model_optimize.add(Activation('sigmoid'))\n",
    "model_optimize.add(Dense(200))\n",
    "model_optimize.add(Activation('sigmoid'))\n",
    "model_optimize.add(Dense(10))\n",
    "model_optimize.add(Activation('softmax'))\n",
    "model_optimize.compile(loss='mse',optimizer=SGD(lr = 0.02),metrics=['accuracy'])\n",
    "model_optimize.summary()\n",
    "model_optimize.fit(x_train,y_train,batch_size=10,epochs=60,verbose = 1,validation_data = (x_test,y_test))\n",
    "score = model_optimize）.evaluate(x_test,y_test)\n",
    "print(\"---------Training accuracy = \",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最後準確率可以高達95.5％"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_MNIST = model_optimize.to_json()\n",
    "open('model_MNIST95.json', 'w').write(model_MNIST)\n",
    "model_optimize.save_weights('model_MNIST95.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
