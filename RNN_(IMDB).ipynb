{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用RNN來做電影評論\"情義分析\",可以從評論得知對於該電影為\"正評\"或\"負評\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=tensorflow\n"
     ]
    }
   ],
   "source": [
    "%env KERAS_BACKEND=tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "a73ce944-1f06-4b48-91eb-da5ff43936f9"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀入 IMDB 電影數據庫\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train0,y_train0),(x_test0,y_test0) = imdb.load_data(num_words = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料除存以數字編碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 17,\n",
       " 6,\n",
       " 194,\n",
       " 337,\n",
       " 7,\n",
       " 4,\n",
       " 204,\n",
       " 22,\n",
       " 45,\n",
       " 254,\n",
       " 8,\n",
       " 106,\n",
       " 14,\n",
       " 123,\n",
       " 4,\n",
       " 2,\n",
       " 270,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 732,\n",
       " 2098,\n",
       " 101,\n",
       " 405,\n",
       " 39,\n",
       " 14,\n",
       " 1034,\n",
       " 4,\n",
       " 1310,\n",
       " 9,\n",
       " 115,\n",
       " 50,\n",
       " 305,\n",
       " 12,\n",
       " 47,\n",
       " 4,\n",
       " 168,\n",
       " 5,\n",
       " 235,\n",
       " 7,\n",
       " 38,\n",
       " 111,\n",
       " 699,\n",
       " 102,\n",
       " 7,\n",
       " 4,\n",
       " 4039,\n",
       " 2,\n",
       " 9,\n",
       " 24,\n",
       " 6,\n",
       " 78,\n",
       " 1099,\n",
       " 17,\n",
       " 2345,\n",
       " 2,\n",
       " 21,\n",
       " 27,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 1603,\n",
       " 92,\n",
       " 1183,\n",
       " 4,\n",
       " 1310,\n",
       " 7,\n",
       " 4,\n",
       " 204,\n",
       " 42,\n",
       " 97,\n",
       " 90,\n",
       " 35,\n",
       " 221,\n",
       " 109,\n",
       " 29,\n",
       " 127,\n",
       " 27,\n",
       " 118,\n",
       " 8,\n",
       " 97,\n",
       " 12,\n",
       " 157,\n",
       " 21,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 6,\n",
       " 66,\n",
       " 78,\n",
       " 1099,\n",
       " 4,\n",
       " 631,\n",
       " 1191,\n",
       " 5,\n",
       " 2642,\n",
       " 272,\n",
       " 191,\n",
       " 1070,\n",
       " 6,\n",
       " 2,\n",
       " 8,\n",
       " 2197,\n",
       " 2,\n",
       " 2,\n",
       " 544,\n",
       " 5,\n",
       " 383,\n",
       " 1271,\n",
       " 848,\n",
       " 1468,\n",
       " 2,\n",
       " 497,\n",
       " 2,\n",
       " 8,\n",
       " 1597,\n",
       " 2,\n",
       " 2,\n",
       " 21,\n",
       " 60,\n",
       " 27,\n",
       " 239,\n",
       " 9,\n",
       " 43,\n",
       " 2,\n",
       " 209,\n",
       " 405,\n",
       " 10,\n",
       " 10,\n",
       " 12,\n",
       " 764,\n",
       " 40,\n",
       " 4,\n",
       " 248,\n",
       " 20,\n",
       " 12,\n",
       " 16,\n",
       " 5,\n",
       " 174,\n",
       " 1791,\n",
       " 72,\n",
       " 7,\n",
       " 51,\n",
       " 6,\n",
       " 1739,\n",
       " 22,\n",
       " 4,\n",
       " 204,\n",
       " 131,\n",
       " 9]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train0[24999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練資料長度 = 25000\n",
      "測試資料長度 = 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"訓練資料長度 =\",len(x_train0))\n",
    "print(\"測試資料長度 =\",len(x_test0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train0[24999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train0[9982])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 輸出資料部份\n",
    "\n",
    "輸出方面應該很容易想像, 我們來看看前 10 筆。結果自然就是 0 (負評) 或 1 (正評)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train0[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 送入神經網路的輸入處理\n",
    "\n",
    "雖然 RNN 是可以處理不同長度的輸入, 在寫程式時我們還是要\n",
    "\n",
    "* 設輸入文字長度的上限\n",
    "* 把每段文字都弄成一樣長, 太短的後面補上 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先將影評長度用成一樣(不一定需要,但較簡單)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train0,maxlen = 150)\n",
    "x_test = sequence.pad_sequences(x_test0,maxlen = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train0\n",
    "y_test = y_test0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 決定神經網路架構\n",
    "\n",
    "* 先將 10000 維的文字壓到 N 維\n",
    "* 然後用 K 個 LSTM 神經元做隱藏層\n",
    "* 最後一個 output, 直接用 sigmoid 送出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建構我們的神經網路\n",
    "\n",
    "文字我們用 1-hot 表示是很標準的方式, 不過要注意的是, 因為我們指定要 1 萬個字, 所以每個字是用 1 萬維的向量表示! 這一來很浪費記憶空間, 二來字和字間基本上是沒有關係的。我們可以用某種「合理」的方式, 把字壓到比較小的維度, 這些向量又代表某些意思 (比如說兩個字代表的向量角度小表相關程度大) 等等。\n",
    "\n",
    "這聽來很複雜的事叫 \"word embedding\", 而事實上 Keras 會幫我們做。我們只需告訴 Keras 原來最大的數字是多少 (10000), 還有我們打算壓到幾維 (N)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3 #文字壓成3維\n",
    "K1 = 20 #20神經元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense ,Embedding ,Dropout\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(5000,N))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(LSTM(K1))\n",
    "model.add(Dense(100,activation = 'sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1,activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 組裝\n",
    "\n",
    "這次我們用 binary_crossentropy 做我們的 loss function, 另外用一個很潮的 Adam 學習法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 3)           15000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 3)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20)                1920      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               2100      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 19,121\n",
      "Trainable params: 19,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 103s - loss: 0.5634 - acc: 0.6631   \n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 104s - loss: 0.3307 - acc: 0.8605   \n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 155s - loss: 0.2932 - acc: 0.8797   \n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 156s - loss: 0.2717 - acc: 0.8908   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f71f1146a90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,batch_size = 64,epochs = 4)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
